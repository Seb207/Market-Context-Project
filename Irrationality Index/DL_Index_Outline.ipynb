{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOihlMiaMmZiK6mkhHrXj89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seb207/Market-Context-Project/blob/main/Irrationality%20Index/DL_Index_Outline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Index Outline\n",
        "\n",
        "Outline for deep learning model to analze market factors and output irrationality index.\n",
        "\n",
        "Model: Transformer\n",
        "\n",
        "Data_Format: history data in second\n",
        "\n",
        "- Positional Embedding Class\n",
        "- Transformer Encoder Block Class\n",
        "- Prediction Head Class\n",
        "- Irrationality Index Class"
      ],
      "metadata": {
        "id": "MTQce3MpdVNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis\n",
        "\n",
        "**Factors**\n",
        "- Volatility: VIX (moving avg. - 20 days)\n",
        "- Volume: Trading Volume (periodic average - 20 days)\n",
        "- Fixed Income (macro): US bond rate momentum\n",
        "- Periodicity: past rate of return (20, 60, 120 days)\n",
        "- Small vs. Big performace: (QQQ -> factor: IWM, IWM -> factor: QQQ) rate of return\n",
        "-\n",
        "\n",
        "**Tested Data**\n",
        "- SPY (S&P 500)\n",
        "- QQQ (Nasdaq)\n",
        "- DIA (Dow Jones)\n",
        "- IWM (Russell 2000)\n",
        "- Individual stocks\n",
        "\n",
        "1."
      ],
      "metadata": {
        "id": "U-azsMTEm0MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Problem 1:\n",
        "- As factors have each different absolute values, output could be affected by its absolute value if directly used as input.\n",
        "\n",
        "Solution:\n",
        "- Used data normalization (z-score) to"
      ],
      "metadata": {
        "id": "3uZMrXIAsg4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "hFh0HZGCdWdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pS0K6BZ2c8F1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sklearn as skl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "**Model Architecture**\n",
        "- input_dim: number of factors\n",
        "- seq_len: length of data processed in the model at once\n",
        "- d_model: dimension of the model\n",
        "- num_heads: number of heads for multi-head attention (should be divisor of d_model)\n",
        "- d_ff: inner dimension of feed forward (4x of d_model in usual)\n",
        "- num_layers: number of transformer encoder block (depth of model)\n",
        "- output_dim: output dimension\n",
        "\n",
        "**Training and Normalization**\n",
        "- dropout_rate\n",
        "- learning_rate\n",
        "- batch_size"
      ],
      "metadata": {
        "id": "AaPi-z4H1Tqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder Block Class\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bNvZPjRZkSP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=d_model // num_heads, dropout=dropout_rate\n",
        "        )\n",
        "        self.feed_forward = keras.Sequential([\n",
        "            layers.Dense(d_ff, activation=\"relu\"),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout_layer = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        attn_output = self.attention(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=mask,\n",
        "            training=training,\n",
        "        )\n",
        "        x1 = self.norm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.feed_forward(x1)\n",
        "        ffn_output = self.dropout_layer(ffn_output, training=training)\n",
        "\n",
        "        return self.norm2(x1 + ffn_output)"
      ],
      "metadata": {
        "id": "4sIoy3nmgVLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding Class"
      ],
      "metadata": {
        "id": "gfAA-KoyuqZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        position_indices = tf.range(start=0, limit=sequence_length, delta=1)\n",
        "        angles = tf.range(start=0, limit=d_model, delta=1, dtype=\"float32\")\n",
        "        angles = 1 / tf.pow(10000.0, (angles - angles % 2) / d_model)\n",
        "\n",
        "        position_indices = tf.cast(tf.expand_dims(position_indices, -1), \"float32\")\n",
        "        angles = tf.cast(tf.expand_dims(angles, 0), \"float32\")\n",
        "\n",
        "        positional_encoding = position_indices * angles\n",
        "        positional_encoding = tf.where(tf.range(d_model) % 2 == 0,\n",
        "                                       tf.sin(positional_encoding),\n",
        "                                       tf.cos(positional_encoding))\n",
        "\n",
        "        self.positional_encoding = tf.cast(positional_encoding, \"float32\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.positional_encoding"
      ],
      "metadata": {
        "id": "1vQHozFfutbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Head Class"
      ],
      "metadata": {
        "id": "UswBJtOfuw_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionHead(keras.Model):\n",
        "    def __init__(self, d_model, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense1 = layers.Dense(d_model // 2, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        return self.dense2(x)"
      ],
      "metadata": {
        "id": "JOpQagwOu0vx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Irrationality Index Class"
      ],
      "metadata": {
        "id": "CscGGS6Cu6JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IrrationalityIndex(keras.Model):\n",
        "    def __init__(self, input_dim, d_model, seq_len, num_heads, d_ff, num_layers, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.factor_weights = layers.Dense(input_dim, activation='sigmoid')\n",
        "        self.normalize = layers.Normalization(axis=-1)\n",
        "        self.input_projection = layers.Dense(d_model)\n",
        "        self.pos_embedding = PositionalEmbedding(seq_len, d_model)\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "\n",
        "        self.encoder_blocks = [\n",
        "            TransformerEncoderBlock(d_model, num_heads, d_ff, 0.1)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.prediction_head = PredictionHead(d_model, output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        inputs_scaled = self.normalize(inputs)\n",
        "        weights = self.factor_weights(inputs_scaled)\n",
        "        weighted_inputs = inputs_scaled * weights\n",
        "\n",
        "        x = self.input_projection(weighted_inputs)\n",
        "        x = self.pos_embedding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        attention_mask = tf.cast(tf.math.not_equal(inputs, 0), dtype=tf.float32)\n",
        "        attention_mask = tf.expand_dims(attention_mask, axis=1)\n",
        "\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x, training=training, mask=attention_mask)\n",
        "\n",
        "        context_vector = x\n",
        "        outputs = self.prediction_head(context_vector)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "iM9SVxgvu5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data"
      ],
      "metadata": {
        "id": "q1YiFxWavskV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of parameters\n",
        "input_features = 5\n",
        "seq_length = 30\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 512\n",
        "num_layers = 3\n",
        "output_classes = 2"
      ],
      "metadata": {
        "id": "V-cY6Fa3vuts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = IrrationalityIndex(input_features, d_model, seq_length, num_heads, d_ff, num_layers, output_classes)"
      ],
      "metadata": {
        "id": "f3C0ITagv31L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.random.randn(100, seq_length, input_features).astype(np.float32) # randomly generated for example"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z0jacfR2v7rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.normalize.adapt(train_data) # normalize to avoid error due to difference in absolute values"
      ],
      "metadata": {
        "id": "voBUiRqnwGUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "WFg4GIrqwUqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sALMr8jCwXS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}